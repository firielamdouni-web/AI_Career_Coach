# AI Career Coach - Developer Guide for AI Agents

## Project Overview
AI Career Coach is a semantic CV-to-job matching system for junior Data Science/ML Engineering profiles. It combines NLP, embeddings, ML classification, and vector search to provide personalized job recommendations with explainability.

## Architecture

### Three-Tier System
1. **Frontend**: Streamlit dashboard ([app.py](../app.py)) - makes HTTP calls to API
2. **Backend**: FastAPI REST API ([src/api.py](../src/api.py)) - orchestrates all processing
3. **Database**: PostgreSQL (schema in [init_db.sql](../init_db.sql)) - stores analyses and recommendations

### Core Pipeline Flow
```
CV Upload → Parse (PyPDF2/pdfplumber) → Extract Skills (spaCy + regex) →
Optional FAISS Pre-filter (top-50) → Detailed Scoring (SentenceTransformer) →
Rank & Filter → Return Top-N
```

### Critical Components

**Job Matcher ([src/job_matcher.py](../src/job_matcher.py))**
- Uses `all-mpnet-base-v2` SentenceTransformer for semantic embeddings (768-dim)
- **Scoring "Approach 4"**: `score = (coverage × 0.5) + (quality × 0.5)`
  - Coverage = matched_skills / required_skills
  - Quality = average cosine similarity of matched skills
- Skills are normalized via `variations_map` (e.g., "scikit-learn" → "sklearn")

**Skills Extraction ([src/skills_extractor.py](../src/skills_extractor.py))**
- Reference database: [data/skills_reference.json](../data/skills_reference.json) (171 tech skills + soft skills)
- Uses spaCy `en_core_web_sm` model + regex pattern matching
- Handles special chars (C++, Node.js, .NET) with escaped patterns

**Vector Store ([src/vector_store.py](../src/vector_store.py))**
- FAISS IndexFlatIP for exact cosine similarity search
- Pre-filters candidates when >50 jobs (reduces latency from ~2.5s to 0.5s)
- Embeddings normalized for cosine similarity via inner product

**Interview Simulator ([src/interview_simulator.py](../src/interview_simulator.py))**
- Uses Groq API (Llama 3.1 70B) for question generation
- Requires `GROQ_API_KEY` environment variable

## Model Loading Pattern
API uses singleton pattern to avoid reloading models on every request:
```python
_cv_parser = None
def get_cv_parser() -> CVParser:
    global _cv_parser
    if _cv_parser is None:
        _cv_parser = CVParser(method='pdfplumber')
    return _cv_parser
```
Apply this pattern when adding new model-based services.

## Developer Workflows

### Local Development
```bash
# Install dependencies (includes spaCy model download)
pip install -r requirements.txt
python -m spacy download en_core_web_sm

# Run MLOps pipeline (generates FAISS index & trains XGBoost)
python mlops/train_and_log.py
python mlops/register_model.py

# Start services
mlflow ui --backend-store-uri file:./mlops/mlflow_tracking  # Port 5000
uvicorn src.api:app --reload --port 8000                    # Port 8000
streamlit run app.py                                        # Port 8501
```

### Docker Deployment
```bash
docker-compose build
docker-compose up -d
# Services: postgres:5432, api:8000, streamlit:8501
```

**Important**: Set `GROQ_API_KEY` in environment before running containers.

### Running Notebooks
Notebooks in [notebooks/](../notebooks/) follow numbered sequence (01-09):
- `01_cv_parser.ipynb` → `02_skills_extraction_simple.ipynb` → ... → `09_ml_model_training.ipynb`
- They generate artifacts saved to [data/](../data/) and [models/](../models/)

## Data Flow & File Dependencies

### Critical Data Files
- `data/jobs/jobs_dataset.json` - 25 curated job postings (generated by notebook 04)
- `data/jobs/jobs_faiss.index` + `jobs_embeddings.pkl` - FAISS index (notebook 06)
- `data/skills_reference.json` - Skills vocabulary with variations mapping
- `data/resume_fit_job/processed/v2_dataset_resume_job_fit_processed.xlsx` - ML training data (4,524 samples)

### MLOps Artifacts
- `mlops/mlflow_tracking/` - MLflow experiment runs (gitignored)
- `models/classifier_clean_metadata.json` - XGBoost model metadata
- XGBoost model: 15 features, 3 classes (No Fit, Partial Fit, Perfect Fit), ~70% accuracy

## Project Conventions

### API Response Format
All endpoints return detailed results with explainability:
```json
{
  "score": 78.3,
  "matching_skills": ["python", "pandas", "numpy"],
  "missing_skills": ["spark", "airflow"],
  "skills_details": {
    "coverage": 76.5,
    "quality": 80.1
  }
}
```

### Skills Normalization
Always lowercase and normalize via `_normalize_skill()` in job_matcher.py:
- Maps variations: "scikit learn" → "scikit-learn" → "sklearn"
- Strips hyphens/underscores before matching
- Returns canonical form from variations_map

### Error Handling
- Missing spaCy model → RuntimeError with download instructions
- Missing GROQ_API_KEY → ValueError with signup link
- Missing data files → FileNotFoundError with notebook reference

## Testing
Tests live in [tests/](../tests/). Current coverage is minimal - when adding features, write tests in this directory following pytest conventions.

## Known Quirks

1. **spaCy Language Mismatch**: Code uses `en_core_web_sm`, but [Dockerfile.api](../Dockerfile.api) downloads `fr_core_news_lg`. Check language requirements before deploying.

2. **FAISS Indexing**: Index is only used when `use_faiss=True` OR jobs count >50. For 25-job dataset, it's often bypassed.

3. **Docker Volume Mounts**: `data/`, `models/`, `outputs/` are mounted as volumes - changes persist without rebuild, but new dependencies require rebuild.

4. **PostgreSQL Init**: Database schema is auto-created only on first startup via [init_db.sql](../init_db.sql). To reset, delete `postgres_data` volume.

## Adding New Features

### To add a new skill category:
1. Update [data/skills_reference.json](../data/skills_reference.json)
2. Add variations to `variations` section
3. Update `_build_variations_map()` in [src/job_matcher.py](../src/job_matcher.py)

### To modify scoring algorithm:
1. Update `calculate_job_match_score()` in [src/job_matcher.py](../src/job_matcher.py)
2. Adjust weights in Approach 4 formula or create new approach
3. Re-run [notebooks/05_job_recommendation.ipynb](../notebooks/05_job_recommendation.ipynb) to validate

### To add API endpoints:
1. Define Pydantic models in [src/api.py](../src/api.py)
2. Use singleton getters for expensive resources
3. Add CORS if frontend needs access
4. Update [app.py](../app.py) to consume new endpoint

## Dependencies & Versions
- SentenceTransformer model: `all-mpnet-base-v2` (fixed in JobMatcher/VectorStore)
- spaCy model: `en_core_web_sm` (code) vs `fr_core_news_lg` (Dockerfile) ⚠️
- Python: 3.10 (Docker base image)
- Key libs: transformers, sentence-transformers, faiss-cpu, xgboost, mlflow
