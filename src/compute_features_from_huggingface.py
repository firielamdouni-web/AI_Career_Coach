"""
Script pour calculer les features ML Ã  partir du dataset Hugging Face
Utilise SkillsExtractor et JobMatcher pour extraire les features PERTINENTES
Version optimisÃ©e : 15 features sÃ©lectionnÃ©es + SUPPORT GPU
"""

import sys
from pathlib import Path

# Ajouter le dossier parent au PATH
sys.path.append(str(Path(__file__).parent.parent))

import pandas as pd
import numpy as np
from tqdm import tqdm
import logging
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer
import torch  # âœ… AJOUT

from src.skills_extractor import SkillsExtractor
from src.job_matcher import JobMatcher

# Configuration du logging
logging.basicConfig(
    level=logging.WARNING,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ===== INITIALISATION =====
print("=" * 70)
print("ğŸš€ CALCUL DES FEATURES ML - DATASET HUGGING FACE (VERSION OPTIMISÃ‰E + GPU)")
print("=" * 70)

# âœ… DÃ‰TECTION DU GPU
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"\nğŸ–¥ï¸  Device dÃ©tectÃ© : {device.upper()}")

if device == "cuda":
    gpu_name = torch.cuda.get_device_name(0)
    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
    print(f"   â€¢ GPU : {gpu_name}")
    print(f"   â€¢ VRAM : {gpu_memory:.1f} GB")
    print(f"   âš¡ AccÃ©lÃ©ration GPU activÃ©e !\n")
else:
    print(f"   âš ï¸  Aucun GPU dÃ©tectÃ©, utilisation du CPU")
    print(f"   ğŸ’¡ Installe CUDA pour accÃ©lÃ©rer : https://pytorch.org/get-started/locally/\n")

print("ğŸ”§ Initialisation des modules...")
skills_extractor = SkillsExtractor()
job_matcher = JobMatcher()

# ModÃ¨le pour embedding similarity (avec GPU)
print(f"ğŸ“¦ Chargement du modÃ¨le Sentence-Transformer sur {device.upper()}...")
embedding_model = SentenceTransformer('all-mpnet-base-v2', device=device)  # âœ… MODIFICATION

# Vectorizer pour TF-IDF (CPU seulement)
tfidf_vectorizer = TfidfVectorizer(max_features=500, stop_words='english')

print("âœ… Modules initialisÃ©s avec succÃ¨s\n")

# ===== CHARGEMENT DU DATASET =====
print("ğŸ“¥ Chargement du dataset...")

csv_path = Path('data/huggingface_resume_job_fit.xlsx')

if not csv_path.exists():
    print(f"âŒ Fichier non trouvÃ© : {csv_path}")
    print("ğŸ’¡ ExÃ©cute d'abord le notebook 08_exploration_dataset.ipynb (Cellule 12)")
    sys.exit(1)

df = pd.read_excel(csv_path, engine='openpyxl')

print(f"âœ… Dataset chargÃ© : {len(df):,} samples")

# ===== LIMITATION Ã€ SAMPLE_LIMIT SAMPLES =====
SAMPLE_LIMIT = None

if SAMPLE_LIMIT is not None and len(df) > SAMPLE_LIMIT:
    print(f"\nâš ï¸  Limitation du dataset Ã  {SAMPLE_LIMIT} samples (pour tests)")
    df = df.head(SAMPLE_LIMIT).copy()
    print(f"âœ… Dataset rÃ©duit : {len(df)} samples\n")
else:
    print(f"âœ… Traitement du dataset complet : {len(df):,} samples\n")

# RÃ©initialiser les index pour Ã©viter les bugs
df = df.reset_index(drop=True)
print(f"âœ… Index rÃ©initialisÃ©s (0 Ã  {len(df)-1})\n")

# ===== PRÃ‰-CALCUL DES EMBEDDINGS (OPTIMISATION + GPU) =====
print(f"ğŸ”„ PrÃ©-calcul des embeddings sur {device.upper()} (pour optimisation)...")

# Extraire les textes
resume_texts = df['resume'].astype(str).tolist()
job_texts = df['job_description'].astype(str).tolist()

# âœ… OPTIMISATION GPU : Batch size augmentÃ©
batch_size = 64 if device == "cuda" else 32  # Plus grand batch sur GPU

# GÃ©nÃ©rer les embeddings par batch (plus rapide)
print(f"   â€¢ Embeddings des CVs (batch={batch_size})...")
resume_embeddings = embedding_model.encode(
    resume_texts, 
    show_progress_bar=True, 
    batch_size=batch_size,
    device=device  # âœ… AJOUT
)

print(f"   â€¢ Embeddings des Jobs (batch={batch_size})...")
job_embeddings = embedding_model.encode(
    job_texts, 
    show_progress_bar=True, 
    batch_size=batch_size,
    device=device  # âœ… AJOUT
)

# âœ… TransfÃ©rer sur CPU pour la suite (Ã©conomie VRAM)
if device == "cuda":
    resume_embeddings = resume_embeddings
    job_embeddings = job_embeddings
    torch.cuda.empty_cache()  # LibÃ©rer la mÃ©moire GPU

print("âœ… Embeddings prÃ©-calculÃ©s\n")

# ===== PRÃ‰-CALCUL DES TF-IDF (OPTIMISATION) =====
print("ğŸ”„ PrÃ©-calcul des TF-IDF...")

# Combiner tous les textes pour fit le vectorizer
all_texts = resume_texts + job_texts
tfidf_vectorizer.fit(all_texts)

# Transformer les textes
resume_tfidf = tfidf_vectorizer.transform(resume_texts)
job_tfidf = tfidf_vectorizer.transform(job_texts)

print("âœ… TF-IDF prÃ©-calculÃ©s\n")

# ===== FONCTION D'EXTRACTION DE FEATURES =====
def compute_features_for_row(row, row_idx):
    """
    Calculer les 15 features ML PERTINENTES pour une paire (CV, Job)
    
    Features calculÃ©es :
    - Skills Matching (5) : coverage, quality, nb_covered_skills, nb_missing_skills, skills_ratio
    - SimilaritÃ© (4) : similarity_mean, similarity_max, similarity_std, top3_similarity_avg
    - SÃ©mantique (2) : tfidf_similarity, embedding_similarity
    - Contexte (4) : nb_resume_technical, nb_resume_soft, nb_job_technical, nb_job_soft
    
    Args:
        row: Ligne du DataFrame pandas
        row_idx: Index de la ligne (pour accÃ¨s aux embeddings)
        
    Returns:
        dict avec les 15 features
    """
    try:
        # ===== 1. EXTRACTION DES SKILLS =====
        cv_result = skills_extractor.extract_from_cv(str(row['resume']))
        cv_technical = cv_result.get('technical_skills', [])
        cv_soft = cv_result.get('soft_skills', [])
        cv_skills_list = cv_technical + cv_soft
        
        job_result = skills_extractor.extract_from_cv(str(row['job_description']))
        job_technical = job_result.get('technical_skills', [])
        job_soft = job_result.get('soft_skills', [])
        job_skills_list = job_technical + job_soft
        
        # ===== 2. FEATURES DE CONTEXTE (4 features) =====
        nb_resume_technical = len(cv_technical)
        nb_resume_soft = len(cv_soft)
        nb_job_technical = len(job_technical)
        nb_job_soft = len(job_soft)
        
        # ===== 3. GESTION DES CAS VIDES =====
        if len(cv_skills_list) == 0 or len(job_skills_list) == 0:
            # Calculer quand mÃªme les similaritÃ©s sÃ©mantiques
            tfidf_sim = cosine_similarity(resume_tfidf[row_idx], job_tfidf[row_idx])[0][0]
            embedding_sim = cosine_similarity(
                resume_embeddings[row_idx].reshape(1, -1),
                job_embeddings[row_idx].reshape(1, -1)
            )[0][0]
            
            return {
                # Skills Matching
                'coverage': 0.0,
                'quality': 0.0,
                'nb_covered_skills': 0,
                'nb_missing_skills': len(job_skills_list),
                'skills_ratio': 0.0,
                
                # SimilaritÃ©
                'similarity_mean': 0.0,
                'similarity_max': 0.0,
                'similarity_std': 0.0,
                'top3_similarity_avg': 0.0,
                
                # SÃ©mantique
                'tfidf_similarity': float(tfidf_sim),
                'embedding_similarity': float(embedding_sim),
                
                # Contexte
                'nb_resume_technical': nb_resume_technical,
                'nb_resume_soft': nb_resume_soft,
                'nb_job_technical': nb_job_technical,
                'nb_job_soft': nb_job_soft
            }
        
        # ===== 4. CALCUL DU MATCHING AVEC JobMatcher =====
        job_structure = {
            'job_id': f'job_{row_idx}',
            'title': 'Job Title',
            'requirements': job_skills_list,
            'nice_to_have': []
        }
        
        match_result = job_matcher.calculate_job_match_score(cv_skills_list, job_structure)
        skills_details = match_result.get('skills_details', {})
        
        # ===== 5. FEATURES SKILLS MATCHING (5 features) =====
        coverage = skills_details.get('coverage', 0.0)
        quality = skills_details.get('quality', 0.0)
        nb_covered_skills = skills_details.get('covered_count', 0)
        total_required = skills_details.get('total_required', len(job_skills_list))
        nb_missing_skills = total_required - nb_covered_skills
        skills_ratio = len(cv_skills_list) / max(len(job_skills_list), 1)
        
        # ===== 6. FEATURES SIMILARITÃ‰ (4 features) =====
        top_matches = skills_details.get('top_matches', [])
        
        if top_matches and len(top_matches) > 0:
            similarities = [m.get('similarity', 0.0) for m in top_matches]
            
            similarity_mean = np.mean(similarities)
            similarity_max = np.max(similarities)
            similarity_std = np.std(similarities)
            
            # Top 3 moyenne
            top3 = sorted(similarities, reverse=True)[:3]
            top3_similarity_avg = np.mean(top3)
        else:
            similarity_mean = 0.0
            similarity_max = 0.0
            similarity_std = 0.0
            top3_similarity_avg = 0.0
        
        # ===== 7. FEATURES SÃ‰MANTIQUES (2 features) =====
        # TF-IDF similarity (depuis prÃ©-calcul)
        tfidf_sim = cosine_similarity(resume_tfidf[row_idx], job_tfidf[row_idx])[0][0]
        
        # Embedding similarity (depuis prÃ©-calcul)
        embedding_sim = cosine_similarity(
            resume_embeddings[row_idx].reshape(1, -1),
            job_embeddings[row_idx].reshape(1, -1)
        )[0][0]
        
        # ===== 8. RETOUR DES 15 FEATURES =====
        return {
            # Skills Matching (5)
            'coverage': float(coverage),
            'quality': float(quality),
            'nb_covered_skills': int(nb_covered_skills),
            'nb_missing_skills': int(nb_missing_skills),
            'skills_ratio': float(skills_ratio),
            
            # SimilaritÃ© (4)
            'similarity_mean': float(similarity_mean),
            'similarity_max': float(similarity_max),
            'similarity_std': float(similarity_std),
            'top3_similarity_avg': float(top3_similarity_avg),
            
            # SÃ©mantique (2)
            'tfidf_similarity': float(tfidf_sim),
            'embedding_similarity': float(embedding_sim),
            
            # Contexte (4)
            'nb_resume_technical': int(nb_resume_technical),
            'nb_resume_soft': int(nb_resume_soft),
            'nb_job_technical': int(nb_job_technical),
            'nb_job_soft': int(nb_job_soft)
        }
    
    except Exception as e:
        logger.error(f"âš ï¸ Erreur ligne {row_idx} : {e}")
        
        # Retourner features par dÃ©faut
        return {
            'coverage': 0.0,
            'quality': 0.0,
            'nb_covered_skills': 0,
            'nb_missing_skills': 0,
            'skills_ratio': 0.0,
            'similarity_mean': 0.0,
            'similarity_max': 0.0,
            'similarity_std': 0.0,
            'top3_similarity_avg': 0.0,
            'tfidf_similarity': 0.0,
            'embedding_similarity': 0.0,
            'nb_resume_technical': 0,
            'nb_resume_soft': 0,
            'nb_job_technical': 0,
            'nb_job_soft': 0
        }

# ===== CALCUL DES FEATURES POUR TOUT LE DATASET =====
estimated_time = len(df) * 0.5 / 60 if device == "cuda" else len(df) * 1.0 / 60
print(f"ğŸ”„ Calcul des 15 features ML pour {len(df):,} samples...")
print(f"   â±ï¸ Temps estimÃ© : ~{estimated_time:.1f} minutes ({device.upper()})\n")

features_list = []
errors_count = 0

for seq_idx, (_, row) in enumerate(tqdm(df.iterrows(), total=len(df), desc="ğŸ“Š Traitement", unit="sample")):
    features = compute_features_for_row(row, seq_idx)  # âœ… seq_idx garanti de 0 Ã  N-1
    
    # Compter les erreurs
    if features['coverage'] == 0.0 and features['quality'] == 0.0:
        errors_count += 1
    
    features_list.append(features)

# ===== CRÃ‰ATION DU DATAFRAME FINAL =====
print(f"\nâœ… Traitement terminÃ©")
print(f"   â€¢ Samples traitÃ©s : {len(features_list):,}")
print(f"   â€¢ Erreurs/vides   : {errors_count:,} ({errors_count/len(df)*100:.1f}%)\n")

features_df = pd.DataFrame(features_list)
features_df['score_target'] = df['score_target'].values

print(f"ğŸ“Š Features calculÃ©es :")
print(f"   â€¢ Nombre de features : {len(features_df.columns) - 1}")
print(f"   â€¢ Features : {[col for col in features_df.columns if col != 'score_target']}\n")

# ===== VÃ‰RIFICATION DE LA QUALITÃ‰ =====
print("ğŸ” VÃ©rification de la qualitÃ© des donnÃ©es...\n")

# Valeurs manquantes
missing = features_df.isnull().sum()
if missing.sum() == 0:
    print("âœ… Aucune valeur manquante")
else:
    print("âš ï¸ Valeurs manquantes dÃ©tectÃ©es :")
    print(missing[missing > 0])

# Statistiques descriptives
print(f"\nğŸ“Š Statistiques descriptives :")
print(features_df.describe())

# CorrÃ©lations avec target
print(f"\nğŸ“Š CorrÃ©lations avec score_target :")
correlations = features_df.corr()['score_target'].drop('score_target').sort_values(ascending=False)
print(correlations)

# Top 3 features corrÃ©lÃ©es
print(f"\nğŸ¯ Top 3 features les plus corrÃ©lÃ©es :")
for i, (feat, corr) in enumerate(correlations.head(3).items(), 1):
    print(f"   {i}. {feat:25} â†’ {corr:.3f}")

# ===== SAUVEGARDE =====
output_path = Path('data/ml_features_optimized_v2.csv')
output_path.parent.mkdir(parents=True, exist_ok=True)

features_df.to_csv(output_path, index=False)

print(f"\nâœ… Dataset avec features sauvegardÃ© : {output_path}")
print(f"   â€¢ Taille         : {len(features_df):,} samples")
print(f"   â€¢ Features       : {len(features_df.columns) - 1}")
print(f"   â€¢ Taille fichier : {output_path.stat().st_size / 1024:.2f} KB")

# ===== RÃ‰SUMÃ‰ FINAL =====
print("\n" + "=" * 70)
print("ğŸ¯ RÃ‰SUMÃ‰ DU TRAITEMENT")
print("=" * 70)
print(f"âœ… Device utilisÃ©    : {device.upper()}")
print(f"âœ… Dataset source    : {len(df):,} samples")
print(f"âœ… Features extraites : 15 (optimisÃ©es)")
print(f"âœ… Samples finaux     : {len(features_df):,}")
print(f"âœ… Taux de rÃ©ussite   : {(len(df) - errors_count) / len(df) * 100:.1f}%")
print(f"\nğŸ“Š Features calculÃ©es :")
print(f"   â€¢ Skills Matching (5) : coverage, quality, nb_covered_skills, nb_missing_skills, skills_ratio")
print(f"   â€¢ SimilaritÃ© (4)      : similarity_mean, similarity_max, similarity_std, top3_similarity_avg")
print(f"   â€¢ SÃ©mantique (2)      : tfidf_similarity, embedding_similarity")
print(f"   â€¢ Contexte (4)        : nb_resume_technical, nb_resume_soft, nb_job_technical, nb_job_soft")
print(f"\nğŸ“‚ Fichier sauvegardÃ© : {output_path}")
print(f"\nğŸ¯ Prochaine Ã©tape    : EntraÃ®ner le modÃ¨le ML")
print("=" * 70)